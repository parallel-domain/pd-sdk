{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data loader for a Segmentation model\n",
    "Using the PD SDK within your data pipeline gives you a couple of advantages:\n",
    "- only one data loader is needed for all dataset formats supported by PD SDK\n",
    "- it's easy to mix data from different dataset formats since the model representation is shared\n",
    "- it will work with datasets located locally or in s3 (this is also easily extendable to more cloud storage providers. See AnyPath)\n",
    "\n",
    "In the following snippet we show an example on how to create a generator that will yield you image + segmentation mask tuples using the PD SDK.\n",
    "This generator yields examples from a PD dgp dataset as well as from the NuImages train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, Dict, List, Any, Generator\n",
    "import numpy as np\n",
    "\n",
    "from paralleldomain.decoding.helper import decode_dataset\n",
    "from paralleldomain.model.annotation import AnnotationTypes\n",
    "from paralleldomain.utilities.mask import replace_values\n",
    "from paralleldomain.model.class_mapping import LabelMapping, OnLabelNotDefined\n",
    "from paralleldomain.utilities.any_path import AnyPath\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetReference:\n",
    "    dataset_path: AnyPath\n",
    "    label_map: Dict[str, str]\n",
    "    dataset_format: str = \"dgp\"\n",
    "    decoder_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "def data_generator(\n",
    "    dataset_references: List[DatasetReference], class_name_to_index: Dict[str, int]\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    for dataset_reference in dataset_references:\n",
    "        # We use this to map from the dataset specific class names to a common class name shared between all datasets\n",
    "        label_mapping = LabelMapping(\n",
    "            label_mapping=dataset_reference.label_map, on_not_defined=OnLabelNotDefined.KEEP_LABEL\n",
    "        )\n",
    "\n",
    "        dataset = decode_dataset(\n",
    "            dataset_path=dataset_reference.dataset_path,\n",
    "            dataset_format=dataset_reference.dataset_format,\n",
    "            **dataset_reference.decoder_kwargs\n",
    "        )\n",
    "\n",
    "        # since we dont require temporally ordered frames we can use unordered_scene_names instead of just scene_names.\n",
    "        # All temporally ordered scenes are contained in this superset of names.\n",
    "        for scene_name in dataset.unordered_scene_names:\n",
    "            scene = dataset.get_unordered_scene(scene_name=scene_name)\n",
    "\n",
    "            # Get the class map that maps from class_id to the respective class name\n",
    "            class_map = scene.get_class_map(annotation_type=AnnotationTypes.SemanticSegmentation2D)\n",
    "\n",
    "            # we concatenate those to maps to get a adjusted ClassMap that maps from class_id to a common class name\n",
    "            common_names_class_map = label_mapping @ class_map\n",
    "            internal_to_common_id_map: Dict[int, int] = dict()\n",
    "            for source_id, class_detail in common_names_class_map.items():\n",
    "                if class_detail.name in class_name_to_index:\n",
    "                    internal_to_common_id_map[source_id] = class_name_to_index[class_detail.name]\n",
    "                else:\n",
    "                    internal_to_common_id_map[source_id] = 0  # background\n",
    "\n",
    "            for camera_name in scene.camera_names:\n",
    "                camera = scene.get_camera_sensor(camera_name=camera_name)\n",
    "                for frame_id in camera.frame_ids:\n",
    "                    camera_frame = camera.get_frame(frame_id=frame_id)\n",
    "                    if AnnotationTypes.SemanticSegmentation2D in camera_frame.available_annotation_types:\n",
    "                        rgb = camera_frame.image.rgb\n",
    "                        semseg_annotation = camera_frame.get_annotations(\n",
    "                            annotation_type=AnnotationTypes.SemanticSegmentation2D\n",
    "                        )\n",
    "                        class_ids = semseg_annotation.class_ids\n",
    "                        mapped_class_ids = replace_values(\n",
    "                            mask=class_ids, value_map=internal_to_common_id_map, value_min=0, value_max=255\n",
    "                        )\n",
    "                        yield rgb, mapped_class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the below pseudo code snippet we use the above defined generator to fuse a dgp and a NuImages dataset to train a segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "\n",
    "dataset_references = [\n",
    "    DatasetReference(\n",
    "        dataset_path=AnyPath(\"s3://path/to/my/synthetic/data\"),\n",
    "        label_map={\"Car\": \"car\", \"Bicyclist\": \"cyclist\", \"Bicycle\": \"cyclist\", \"Pedestrian\": \"pedestrian\"},\n",
    "        dataset_format=\"dgp\",\n",
    "    ),\n",
    "    DatasetReference(\n",
    "        dataset_path=AnyPath(\"s3://path/to/my/nuimages/data\"),\n",
    "        label_map={\n",
    "            \"vehicle.car\": \"car\",\n",
    "            \"vehicle.bicycle\": \"cyclist\",\n",
    "            \"human.pedestrian.adult\": \"pedestrian\",\n",
    "            \"human.pedestrian.child\": \"pedestrian\",\n",
    "            \"human.pedestrian.construction_worker\": \"pedestrian\",\n",
    "            \"human.pedestrian.personal_mobility\": \"pedestrian\",\n",
    "            \"human.pedestrian.police_officer\": \"pedestrian\",\n",
    "            \"human.pedestrian.stroller\": \"pedestrian\",\n",
    "            \"human.pedestrian.wheelchair\": \"pedestrian\",\n",
    "        },\n",
    "        dataset_format=\"nuimages\",\n",
    "        decoder_kwargs=dict(split_name=\"v1.0-train\"),\n",
    "    ),\n",
    "]\n",
    "\n",
    "generator = data_generator(\n",
    "    dataset_references=dataset_references, class_name_to_index={\"car\": 0, \"cyclist\": 1, \"pedestrian\": 2}\n",
    ")\n",
    "\n",
    "# Some model train pseudo code to show how to use the generator\n",
    "model = lambda rgb: np.random.rand(*rgb.shape)\n",
    "optimizer = mock.MagicMock()\n",
    "loss_function = lambda pred, cid: np.random.random()\n",
    "\n",
    "for rgb, class_ids in generator:\n",
    "    prediction = model(rgb, class_ids)\n",
    "    loss = loss_function(prediction, class_ids)\n",
    "    optimizer.compute_gradient(loss)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
